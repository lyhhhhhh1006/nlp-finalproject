# 580-project

## Project Summary
##### This project aims to build better abstractive text summarization models for different types of news. Two parts consist of this project. The first one is news classification. In this part,  a news classification model was fine-tuned from the pretrained model ‘DistilBERT-base-cased’ by AGnews dataset. And the accuracy of the testing set is about 94.29%. Then the trained news classification model was used to assign class labels for dataset CNN Dailymail, and the news were divided into four classes(' World’, ‘Sports’, ‘Business’, ‘Sci/Tech’). The second part is text summarization. In this part, four text summarization models were fine-tuned from the pretrained model ‘google/mT5-small’. In order to build more appropriate summarization models for different news types, each class news was used to train the specific model for the corresponding news type. In the end, the average Rouge score for four models are Rouge1:27.26%, Rouge2:12.36%, RougeL: 21.63%. This result is better than the compared model ‘mt5-base-finetuned-en-cnn’, which was fine-tuned from the pretrained model ‘google/mT5-basel’ by the entire CNN Dailymail dataset. Moreover, this project proposed a new method to measure the performance of text summarization tasks. The following plot shows the confusion matrix heat map of the summarization model results’ labels and News original highlights labels. The accuracy of our model is about 70.72%, which means 70.72% News in the testing set still keeps the original class after summarization. This result is better than the compared model’s 68.95% accuracy, especially for business news, our model improved the accuracy by about 4%.
![alt text](https://github.com/SiriRRR/580-project/blob/main/img/img1.png)

## Changes from Proposal
##### During the process of work, we changed the methods mentioned in the proposal. In the proposal, we plan to build a ‘BERT-ITPT-FiT’ news classification model, and build ‘BERTSUM’ text summarization model. However, we found that our computer hardware is not able to run two complex models in a given time. Therefore, in order to balance the work of two parts, we decided to fine-tune from the pretrained model ‘google/mT5-small’ and model ‘DistilBERT-base-cased’.
